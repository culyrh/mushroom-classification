
<h1 align="center">ğŸ„ Mushroom Classification â€” Machine Learning HW1</h1>
<p align="center"><b>Binary Classification of Edible vs Poisonous Mushrooms</b></p>

---

> âš ï¸ <b>WARNING</b>  
> ë³¸ í”„ë¡œì íŠ¸ëŠ” ì™¸í˜•ì  featureë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë²„ì„¯ì˜ ë…ì„± ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì…ë‹ˆë‹¤.  
> <b>ì ˆëŒ€ ì‹¤ì œ ì„­ì·¨ ì—¬ë¶€ íŒë‹¨ì— ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.</b>

---

# Project Overview

ì´ í”„ë¡œì íŠ¸ëŠ” ë²„ì„¯ì˜ í˜•íƒœÂ·ìƒ‰ìƒÂ·í‘œë©´Â·ê³„ì ˆ ë“± **20ê°œì˜ feature**ë¥¼ í™œìš©í•´  
ë²„ì„¯ì´ **ë¨¹ì„ ìˆ˜ ìˆëŠ”ì§€(edible)** ë˜ëŠ” **ë…ì„±(poisonous)** ì¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ”  
<b>ì´ì§„ ë¶„ë¥˜ ë¨¸ì‹ ëŸ¬ë‹ ê³¼ì œ</b>ì…ë‹ˆë‹¤.

- Python built-in ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš© (scikit-learn ê¸ˆì§€)
- ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ **ìˆ˜ì‹ë¶€í„° ì§ì ‘ êµ¬í˜„**

---

# Dataset: `mushroom.csv`

> ì „ì²´ feature ì„¤ëª…ì€ `mushroom_meta.txt` ì°¸ê³   

| í•­ëª©            | ë‚´ìš©                                                           |
|-----------------|----------------------------------------------------------------|
| ë°ì´í„° ìˆ˜       | **61,069ê°œ**                                                   |
| í´ë˜ìŠ¤          | `e` = edible, `p` = poisonous                                  |
| Feature ê°œìˆ˜    | ì´ 20ê°œ                                                        |
| Feature íƒ€ì…    | **17ê°œ ëª…ëª©í˜•(nominal)** + **3ê°œ ìˆ˜ì¹˜í˜•(metrical)**           |
| ë°ì´í„° íŠ¹ì§•     | synthetic dataset (ëœë¤ ê¸°ë°˜)                                   |
| ì£¼ìš” ë³€ìˆ˜       | cap, gill, stem, veil, color, habitat, season ë“± 20ê°œ ì†ì„±     |

### Feature ì˜ˆì‹œ ìš”ì•½  
- `cap-diameter` (m): ëª¨ì ì§€ë¦„  
- `cap-shape` (n): bell, conical, convex, flat ë“±  
- `cap-surface`, `stem-surface`: scaly, fibrous, smooth ë“±  
- `cap-color`, `gill-color`, `stem-color`: ë‹¤ì–‘í•œ ìƒ‰ìƒ ì½”ë“œ  
- `habitat`: woods, meadows, urban ë“±  
- `season`: spring, summer, autumn, winter  

ëª…ëª©í˜• featureëŠ” ëª¨ë‘ **1ê¸€ì ì½”ë“œ**ë¡œ ì¸ì½”ë”©ë˜ì–´ ìˆì–´ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë§¤í•‘ì´ í•„ìš”í•©ë‹ˆë‹¤.

---

# Implemented Algorithms (From Scratch)

ë³¸ í”„ë¡œì íŠ¸ì—ëŠ” ë‹¤ìŒ 5ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ì´ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.

---

## âœ”ï¸ Accuracy Summary

| Algorithm        | Accuracy |
|------------------|----------|
| **kNN (k=5)**    | **0.8819** |
| Naive Bayes      | 0.8039 |
| Decision Tree    | 0.5549 |
| Neural Network   | 0.5549 |
| SVM (Linear)     | 0.5417 |

> kNNì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡ (synthetic + ë²”ì£¼í˜• ë¹„ì¤‘ ë†’ì€ ë°ì´í„° íŠ¹ì„±)

---

# Algorithm Descriptions

## 1. Decision Tree (ID3)
- Entropy / Information Gain ê¸°ë°˜ ë¶„ê¸°  
- ëª…ëª©í˜• featureì— ëŒ€í•´ valueë³„ split  
- Tree Depth ì œí•œ ì ìš©  
- ë°ì´í„° ëœë¤ì„±ì´ ë†’ì•„ ì¼ë°˜í™” ì„±ëŠ¥ì€ ì œí•œë¨

---

## 2ï¸. k-Nearest Neighbors (kNN)
- ê±°ë¦¬ ê¸°ë°˜ ë¶„ë¥˜(Euclidean)  
- ëª…ëª©í˜• feature â†’ ì¼ì¹˜ ì—¬ë¶€ 0/1 ì²˜ë¦¬  
- k = 5 ì„¤ì •  
- ì‹¤í—˜ ê²°ê³¼ ê°€ì¥ ë†’ì€ accuracy ë‹¬ì„±  

---

## 3. Naive Bayes
- Feature ë…ë¦½ ê°€ì •  
- ëª…ëª©í˜• feature â†’ ë¹ˆë„ ê¸°ë°˜ í™•ë¥  + Laplace smoothing  
- ìˆ˜ì¹˜í˜• feature â†’ Gaussian  
- ë§¤ìš° ë¹ ë¥´ê³  baseline ì„±ëŠ¥ ìš°ìˆ˜  

---

## 4. Neural Network (2-layer MLP)
- hidden layer 16, activation = sigmoid  
- binary cross-entropy  
- backpropagation ì§ì ‘ êµ¬í˜„  
- synthetic ë°ì´í„° íŠ¹ì„±ìƒ ë†’ì€ ì„±ëŠ¥ì€ ì–´ë ¤ì›€  

---

## 5. Support Vector Machine (Linear SVM)
- Hinge loss + Gradient Descent  
- Kernel ë¯¸ì‚¬ìš©(ì§ì ‘ êµ¬í˜„ ë‚œì´ë„ ê³ ë ¤)  
- ì„ í˜• ë¶„ë¦¬ ì–´ë ¤ìš´ ë°ì´í„° â†’ ë‚®ì€ accuracy  

---